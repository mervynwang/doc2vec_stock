# -*- coding: utf-8 -*-
"""dl-for-nlp-pytorch-torchtext.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/ariepratama/python-playground/blob/master/dl-for-nlp-pytorch-torchtext.ipynb

# Deep Learning For NLP with PyTorch and Torchtext

This is the companion code for my article in medium. There will be no further explanation here, just pure code.
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.optim import Adam

from torchtext.data import Field
from torchtext.data import Dataset, Example
from torchtext.data import BucketIterator
from torchtext.vocab import FastText
from torchtext.vocab import CharNGram

import pandas as pd
import numpy as np

embedding = FastText('simple')

# embedding_charngram = CharNGram()

frame = pd.read_csv('./data/news_usat.csv', index_col=None, header=0)
cols = [ 'source', 'date', 'ticker',
            'title', 'content_fp', '0dr',
            '1dr', '7dr', '30dr',
            '1d', '7d', '30d',
            '1dt', '7dt', '30dt']
start_date = '2013-01-01'
end_date = '2020-06-30'
df = frame[cols]
df.set_index('date')
df['date'] = pd.to_datetime(df['date'])
mask = (df['date'] > start_date) & (df['date'] <= end_date)
df = df.loc[mask]
df.sort_values(by='date', inplace=True, ascending=True)

df = df.loc[:, ['title','30dt']]
df = df.rename(columns={"30dt": "label"})

df['label'] = df['label'].str.replace('cp', '1')
df['label'] = df['label'].str.replace('p', '0')
df['label'] = df['label'].str.replace('n', '2')
df['label'] = df['label'].str.replace('co', '3')
df['label'] = df['label'].str.replace('o', '4')

print(df.head)

print("-------------------")

text_field = Field(
    sequential=True,
    tokenize='basic_english',
    fix_length=100,
    lower=True
)

label_field = Field(sequential=False, use_vocab=False)


# sadly have to apply preprocess manually
preprocessed_text = df['title'].apply(
    lambda x: text_field.preprocess(x)
)
print("-------------------")


# path='cifar-100/cifar-100-python/pylearn2_gcn_whitened/test/test.npy'
# sample_test = torch.from_numpy(np.load(path))
# from torchtext.vocab import Vectors
# vectors = Vectors(name='xxx.vec', cache='./')
# TEXT.build_vocab(train, val, test, vectors=vectors)

# load fastext simple embedding with 300d
text_field.build_vocab(
    preprocessed_text,
    vectors='fasttext.simple.300d'
)

# get the vocab instance
vocab = text_field.vocab

# known token, in my case print 12
# print(vocab['are'])
# # unknown token, will print 0
# print(vocab['crazy'])

# print("-------------------")
# for i, r in df.iterrows():
#     print(list(r.values))

# we still have to manually handle conversion from categorical to int
print("-------------------")
# print(df['30dt'].unique())
print(df['label'].unique())
print("-------------------")

ltoi = {l: i for i, l in enumerate(df['label'].unique())}
df['label'] = df['label'].apply(lambda y: ltoi[y])

class DataFrameDataset(Dataset):
    def __init__(self, df: pd.DataFrame, fields: list):
        super(DataFrameDataset, self).__init__(
            [
                Example.fromlist(list(r), fields)
                for i, r in df.iterrows()
            ],
            fields
        )

train_dataset, test_dataset = DataFrameDataset(
    df=df,
    fields=(
        ('title', text_field),
        ('label', label_field)
    )
).split()

train_iter, test_iter = BucketIterator.splits(
    datasets=(train_dataset, test_dataset),
    batch_sizes=(2, 2),
    sort=False
)


def calc_accuracy(X,Y):
    """Calculates model accuracy

    Arguments:
        X {torch.Tensor} -- input data
        Y {torch.Tensor} -- labels/target values

    Returns:
        [torch.Tensor] -- accuracy
    """
    max_vals, max_indices = torch.max(X,1)
    n = max_indices.size(0) #index 0 for extracting the # of elements
    train_acc = (max_indices == Y).sum(dtype=torch.float32)/n
    return train_acc

class ModelParam(object):
    def __init__(self, param_dict: dict = dict()):
        self.input_size = param_dict.get('input_size', 0)
        self.vocab_size = param_dict.get('vocab_size')
        self.embedding_dim = param_dict.get('embedding_dim', 600)
        self.target_dim = param_dict.get('target_dim', 5)

class MyModel(nn.Module):
    def __init__(self, model_param: ModelParam):
        super().__init__()
        self.embedding = nn.Embedding(
            model_param.vocab_size,
            model_param.embedding_dim
        )
        self.lin = nn.Linear(
            model_param.input_size * model_param.embedding_dim,
            model_param.target_dim
        )

    def forward(self, x):
        features = self.embedding(x).view(x.size()[0], -1)
        features = F.relu(features)
        features = self.lin(features)
        return features

class MyModelWithPretrainedEmbedding(nn.Module):
    def __init__(self, model_param: ModelParam, embedding):
        super().__init__()
        self.embedding = embedding
        self.lin = nn.Linear(
            model_param.input_size * model_param.embedding_dim,
            model_param.target_dim
        )

    def forward(self, x):
        features = self.embedding[x].reshape(x.size()[0], -1)
        features = F.relu(features)
        features = self.lin(features)
        return features

model_param = ModelParam(
    param_dict=dict(
        vocab_size=len(text_field.vocab),
        input_size=100
    )
)
model = MyModel(model_param)
loss_function = nn.CrossEntropyLoss()
optimizer = Adam(model.parameters(), lr=0.01)
epochs = 8


for epoch in range(epochs):
    print("epoch ------  %d" % epoch)
    epoch_losses = list()
    correct = 0
    nu = 0
    for batch in train_iter:
        optimizer.zero_grad()

        prediction = model(batch.title.T)
        loss = loss_function(prediction, batch.label)
        loss.backward()
        optimizer.step()

        correct = correct + calc_accuracy(prediction, batch.label)
        nu = nu + 1
        # print("acc%s ", (correct/nu) )

        epoch_losses.append(loss.item())

    print("train acc : %d " % (100 * correct/len(train_iter)))
    print('train loss on epoch {} : {:.3f}'.format(epoch, np.mean(epoch_losses)))

    test_losses = list()
    correct = 0
    for batch in test_iter:
        with torch.no_grad():
            optimizer.zero_grad()
            prediction = model(batch.title.T)
            loss = loss_function(prediction, batch.label)
            correct = correct + calc_accuracy(prediction, batch.label)
            test_losses.append(loss.item())

    print("test acc : %d " % (100 * correct/len(test_iter)) )
    print('test loss on epoch {}: {:.3f}'.format(epoch, np.mean(test_losses)))