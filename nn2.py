# -*- coding: utf-8 -*-
"""dl-for-nlp-pytorch-torchtext.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/ariepratama/python-playground/blob/master/dl-for-nlp-pytorch-torchtext.ipynb

# Deep Learning For NLP with PyTorch and Torchtext

This is the companion code for my article in medium. There will be no further explanation here, just pure code.
"""
import torch
from torch.utils.data import DataLoader
from torch.utils.data.dataset import Dataset
import torch.nn as nn

import pandas as pd
import numpy as np
import time
import torch
import torch.nn as nn
import torch.optim as optim
import torch.utils.data as data

from torchtext.data import Field

device = 'cuda:0' if torch.cuda.is_available() else 'cpu'
print('GPU State:', device)

print("-------------------")
tokenize = lambda x: x.split()
TEXT = Field(sequential=True, tokenize=tokenize, lower=True)
print("-------------------")

class newsDataset(Dataset):
    def __init__(self, train):
        self.train = train
        self.data = []
        self.labels = []

        frame = pd.read_csv('./data/news_usat.csv', index_col=None, header=0)
        cols = [ 'source', 'date', 'ticker',
                    'title', 'content_fp', '0dr',
                    '1dr', '7dr', '30dr',
                    '1d', '7d', '30d',
                    '1dt', '7dt', '30dt']
        start_date = '2013-01-01'
        end_date = '2020-06-30'
        df = frame[cols]
        df.set_index('date')
        df['date'] = pd.to_datetime(df['date'])
        mask = (df['date'] > start_date) & (df['date'] <= end_date)
        df = df.loc[mask]
        df.sort_values(by='date', inplace=True, ascending=True)

        df = df.loc[:, ['title','30dt']]
        df = df.rename(columns={"30dt": "label"})

        df = df.head()
        for index, row in df.iterrows():
            print("index %d : %s " % (index,row))
            # to v
            self.data.append(row['title'])
            self.labels.append(row['label'])

        # List convert to tensor
        print(self.data)

        self.data = torch.tensor(self.data).float()
        self.labels = torch.tensor(self.labels).float()

    def __getitem__(self, index):
        return (self.data[index], self.labels[index])

    def __len__(self):
        return self.labels.shape[0]

print("-------------------")


# Model
class fully_connected_model(nn.Module):
    def __init__(self):
        super(fully_connected_model, self).__init__()
        self.main = nn.Sequential(
            nn.Linear(5000, 2048),
            nn.ReLU(),
            nn.Linear(2048, 1024),
            nn.ReLU(),
            nn.Linear(1024, 256),
            nn.ReLU(),
            nn.Linear(256, 16),
            nn.ReLU(),
            nn.Linear(16, 4),
            nn.Sigmoid()
        )

    def forward(self, input):
        return self.main(input)

# Loss
def loss_function(inputs, targets):
    return nn.BCELoss()(inputs, targets)


# Model
model = fully_connected_model().to(device)
print(model)


# Settings
epochs = 20
lr = 0.002
batch_size = 16
optimizer = optim.Adam(model.parameters(), lr=lr)


# DataLoader
train_set = newsDataset(train=True)
print(train_set)
# train_loader = data.DataLoader(train_set, batch_size=batch_size, shuffle=True)


# Train

# for epoch in range(epochs):
#     epoch += 1

#     for times, data in enumerate(train_loader):
#         times += 1
#         inputs = data[0].to(device)
#         labels = data[1].to(device)

#         # Zero gradients
#         optimizer.zero_grad()

#         # Forward & Backward
#         outputs = model(inputs).to(device)
#         loss = loss_function(outputs, labels)
#         loss.backward()
#         optimizer.step()

#         # Display loss
#         if times % 100 == 0 or times == len(train_loader):
#             print('[{}/{}, {}/{}] loss: {:.3f}'.format(epoch, epochs, times, len(train_loader), loss.item()))


# print('Training Finished.')

# Saved
# torch.save(model, 'fc.pth')
# print('Model saved.')