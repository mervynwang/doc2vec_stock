# -*- coding: utf-8 -*-
"""dl-for-nlp-pytorch-torchtext.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/ariepratama/python-playground/blob/master/dl-for-nlp-pytorch-torchtext.ipynb

# Deep Learning For NLP with PyTorch and Torchtext

This is the companion code for my article in medium. There will be no further explanation here, just pure code.
"""
import torch
from torch.utils.data import DataLoader
from torch.utils.data.dataset import Dataset
import torch.nn as nn

import torch.nn.functional as F

import pandas as pd
import numpy as np
import time
import torch
import torch.nn as nn
import torch.optim as optim
import torch.utils.data as data

# from torchtext.data import Field

from gensim.models.doc2vec import Doc2Vec



device = 'cuda:0' if torch.cuda.is_available() else 'cpu'
print('GPU State:', device)

print("-------------------")
tokenize = lambda x: x.split()
# TEXT = Field(sequential=True, tokenize=tokenize, lower=True)
print("-------------------")

class newsDataset(Dataset):
    def __init__(self, train):
        doc2vec = Doc2Vec.load("./vecs/usat_doc2vec")
        self.train = train
        self.data = []
        self.labels = []

        frame = pd.read_csv('./data/news_usat.csv', index_col=None, header=0)
        cols = [ 'source', 'date', 'ticker',
                    'title', 'content_fp', '0dr',
                    '1dr', '7dr', '30dr',
                    '1d', '7d', '30d',
                    '1dt', '7dt', '30dt']
        start_date = '2013-01-01'
        end_date = '2020-06-30'
        df = frame[cols]
        df.set_index('date')
        df['date'] = pd.to_datetime(df['date'])
        mask = (df['date'] > start_date) & (df['date'] <= end_date)
        df = df.loc[mask]
        df.sort_values(by='date', inplace=True, ascending=True)

        df = df.loc[:, ['title','30dt']]
        df = df.rename(columns={"30dt": "label"})

        df = df.head()
        for index, row in df.iterrows():
            print("index %d : %s " % (index,row))

            vec = doc2vec.infer_vector(row['title'].split())
            print("vec %d , title :: %s " %  (len(vec), row['title']))

            self.data.append(vec)
            self.labels.append(row['label'])

        # List convert to tensor
        # print(self.data)
        # pad = torch.tensor(0)

        self.data =  torch.tensor(self.data).float()
        self.labels = torch.tensor(self.labels).float()

    def __getitem__(self, index):
        return (self.data[index], self.labels[index])

    def __len__(self):
        return self.labels.shape[0]

print("-------------------")


# Model
class BaseModel(nn.Module):
    def __init__(self):
        super(BaseModel, self).__init__()
        embedding_dim = 5
        self.model_name = 'BaseModel'

        self.encoder = nn.Embedding(200,embedding_dim)

        self.fc = nn.Linear(embedding_dim, 5)


        self.properties = {"model_name":self.__class__.__name__,
#                "embedding_dim":self.opt.embedding_dim,
#                "embedding_training":self.opt.embedding_training,
#                "max_seq_len":self.opt.max_seq_len,
                "batch_size":2,
                "learning_rate":2e-5,
                "keep_dropout":0.8,
                }

    def forward(self,content):
        content_=torch.mean(self.encoder(content),dim=1)
        out=self.fc(content_.view(content_.size(0),-1))
        return out

# Loss
def loss_function(inputs, targets):
    return nn.BCELoss()(inputs, targets)


# Model
model = BaseModel().to(device)
print(model)


# Settings
epochs = 20
lr = 0.002
batch_size = 16
optimizer = optim.Adam(model.parameters(), lr=lr)


# DataLoader
train_set = newsDataset(train=True)

print("-------------------")
print("--------train_set-----------")
# print(train_set[0])
# print(len(train_set[0][0]))
print("-------------------")
print("-------------------")

train_loader = data.DataLoader(train_set, batch_size=batch_size)


# Train

for epoch in range(epochs):
    epoch += 1

    for times, data in enumerate(train_loader):
        times += 1
        inputs = data[0].to(device)
        labels = data[1].to(device)

        # Zero gradients
        optimizer.zero_grad()

        # Forward & Backward
        outputs = model(inputs).to(device)
        loss = loss_function(outputs, labels)
        loss.backward()
        optimizer.step()

        # Display loss
        if times % 100 == 0 or times == len(train_loader):
            print('[{}/{}, {}/{}] loss: {:.3f}'.format(epoch, epochs, times, len(train_loader), loss.item()))


print('Training Finished.')

Saved
torch.save(model, 'fc.pth')
print('Model saved.')